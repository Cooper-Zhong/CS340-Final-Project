{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2024-04-24T02:00:53.631124Z","iopub.status.busy":"2024-04-24T02:00:53.630786Z","iopub.status.idle":"2024-04-24T02:00:55.689613Z","shell.execute_reply":"2024-04-24T02:00:55.688661Z","shell.execute_reply.started":"2024-04-24T02:00:53.631056Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using TensorFlow backend.\n"]}],"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import datetime\n","import os\n","import pandas as pd\n","import numpy as np\n","import pkg_resources\n","import seaborn as sns\n","import time\n","import scipy.stats as stats\n","import pickle\n","\n","from sklearn import metrics\n","from sklearn import model_selection\n","\n","import tensorflow as tf\n","import keras\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding\n","from keras.layers import Input\n","from keras.layers import Conv1D\n","from keras.layers import MaxPooling1D\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.layers import Dense\n","from keras.optimizers import RMSprop\n","from keras.models import Model\n","from keras.models import load_model"]},{"cell_type":"markdown","metadata":{},"source":["## Load and pre-process the data set"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# List all identities\n","identity_columns = [\n","    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n","    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T02:00:55.693553Z","iopub.status.busy":"2024-04-24T02:00:55.693292Z","iopub.status.idle":"2024-04-24T02:01:23.715538Z","shell.execute_reply":"2024-04-24T02:01:23.714810Z","shell.execute_reply.started":"2024-04-24T02:00:55.693499Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv('./Data/train.csv')\n","print('loaded %d records' % len(train))\n","\n","# Make sure all comment_text values are strings\n","train['comment_text'] = train['comment_text'].astype(str) "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Convert taget and identity columns to booleans\n","def convert_to_bool(df, col_name):\n","    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n","    \n","def convert_dataframe_to_bool(df):\n","    bool_df = df.copy()\n","    for col in ['target'] + identity_columns:\n","        convert_to_bool(bool_df, col)\n","    return bool_df"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["train = convert_dataframe_to_bool(train)"]},{"cell_type":"markdown","metadata":{},"source":["## Split the data into 80% train and 20% validate sets"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T02:01:23.717481Z","iopub.status.busy":"2024-04-24T02:01:23.717163Z","iopub.status.idle":"2024-04-24T02:01:25.493770Z","shell.execute_reply":"2024-04-24T02:01:25.492828Z","shell.execute_reply.started":"2024-04-24T02:01:23.717421Z"},"trusted":true},"outputs":[],"source":["train_df, validate_df = model_selection.train_test_split(train, test_size=0.2)\n","print('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))"]},{"cell_type":"markdown","metadata":{},"source":["## Create a text tokenizer"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T02:01:25.495531Z","iopub.status.busy":"2024-04-24T02:01:25.495199Z","iopub.status.idle":"2024-04-24T02:03:01.848534Z","shell.execute_reply":"2024-04-24T02:03:01.847824Z","shell.execute_reply.started":"2024-04-24T02:01:25.495454Z"},"trusted":true},"outputs":[],"source":["MAX_NUM_WORDS = 10000\n","TOXICITY_COLUMN = 'target'\n","TEXT_COLUMN = 'comment_text'"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# saved tokenizer\n","tokenizer_file = 'models/tokenizer.pickle'\n","\n","if os.path.exists(tokenizer_file):\n","    with open(tokenizer_file, 'rb') as f:\n","        tokenizer = pickle.load(f)\n","else:\n","    # Create a text tokenizer.\n","    tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n","    tokenizer.fit_on_texts(train_df[TEXT_COLUMN])\n","    # 保存Tokenizer到文件\n","    tokenizer_file = 'tokenizer.pickle'\n","    with open(tokenizer_file, 'wb') as f:\n","        pickle.dump(tokenizer, f)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["<keras_preprocessing.text.Tokenizer at 0x7ff5fa04a510>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# All comments must be truncated or padded to be the same length.\n","MAX_SEQUENCE_LENGTH = 250\n","def pad_text(texts, tokenizer):\n","    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)"]},{"cell_type":"markdown","metadata":{},"source":["## Define and train a Convolutional Neural Net for classifying toxic comments"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T02:03:01.850248Z","iopub.status.busy":"2024-04-24T02:03:01.850018Z"},"trusted":true},"outputs":[],"source":["EMBEDDINGS_PATH = './Embedding_file/glove.6B.100d.txt'\n","EMBEDDINGS_DIMENSION = 100\n","DROPOUT_RATE = 0.3\n","LEARNING_RATE = 0.00005\n","NUM_EPOCHS = 10\n","BATCH_SIZE = 128\n","\n","def train_model(train_df, validate_df, tokenizer):\n","    # Prepare data\n","    train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\n","    train_labels = to_categorical(train_df[TOXICITY_COLUMN])\n","    validate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\n","    validate_labels = to_categorical(validate_df[TOXICITY_COLUMN])\n","\n","    # Load embeddings\n","    print('loading embeddings')\n","    embeddings_index = {}\n","    with open(EMBEDDINGS_PATH) as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","\n","    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n","                                 EMBEDDINGS_DIMENSION))\n","    num_words_in_embedding = 0\n","    for word, i in tokenizer.word_index.items():\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            num_words_in_embedding += 1\n","            # words not found in embedding index will be all-zeros.\n","            embedding_matrix[i] = embedding_vector\n","\n","    # Create model layers.\n","    def get_convolutional_neural_net_layers():\n","        \"\"\"Returns (input_layer, output_layer)\"\"\"\n","        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","        embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n","                                    EMBEDDINGS_DIMENSION,\n","                                    weights=[embedding_matrix],\n","                                    input_length=MAX_SEQUENCE_LENGTH,\n","                                    trainable=False)\n","        x = embedding_layer(sequence_input)\n","        x = Conv1D(128, 2, activation='relu', padding='same')(x)\n","        x = MaxPooling1D(5, padding='same')(x)\n","        x = Conv1D(128, 3, activation='relu', padding='same')(x)\n","        x = MaxPooling1D(5, padding='same')(x)\n","        x = Conv1D(128, 4, activation='relu', padding='same')(x)\n","        x = MaxPooling1D(40, padding='same')(x)\n","        x = Flatten()(x)\n","        x = Dropout(DROPOUT_RATE)(x)\n","        x = Dense(128, activation='relu')(x)\n","        preds = Dense(2, activation='softmax')(x)\n","        return sequence_input, preds\n","\n","    # Compile model.\n","    print('compiling model')\n","    input_layer, output_layer = get_convolutional_neural_net_layers()\n","    model = Model(input_layer, output_layer)\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer=RMSprop(lr=LEARNING_RATE),\n","                  metrics=['acc'])\n","\n","    # Train model.\n","    print('training model')\n","    model.fit(train_text,\n","              train_labels,\n","              batch_size=BATCH_SIZE,\n","              epochs=NUM_EPOCHS,\n","              validation_data=(validate_text, validate_labels),\n","              verbose=2)\n","\n","    return model"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["model = train_model(train_df, validate_df, tokenizer)\n","# Save the trained model\n","model.save('./models/baseline_cnn.h5') # keras.models.load_model()"]},{"cell_type":"markdown","metadata":{},"source":["## Generate model predictions on the validation set"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["MODEL_NAME = 'cnn'\n","validate_df[MODEL_NAME] = model.predict(pad_text(validate_df[TEXT_COLUMN], tokenizer))[:, 1]"]},{"cell_type":"markdown","metadata":{},"source":["## Define bias metrics, then evaluate our new model for bias using the validation set predictions"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["SUBGROUP_AUC = 'subgroup_auc'\n","BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n","BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n","\n","def compute_auc(y_true, y_pred):\n","    try:\n","        return metrics.roc_auc_score(y_true, y_pred)\n","    except ValueError:\n","        return np.nan\n","\n","def compute_subgroup_auc(df, subgroup, label, model_name):\n","    subgroup_examples = df[df[subgroup]]\n","    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n","\n","def compute_bpsn_auc(df, subgroup, label, model_name):\n","    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n","    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n","    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n","    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n","    return compute_auc(examples[label], examples[model_name])\n","\n","def compute_bnsp_auc(df, subgroup, label, model_name):\n","    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n","    subgroup_positive_examples = df[df[subgroup] & df[label]]\n","    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n","    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n","    return compute_auc(examples[label], examples[model_name])\n","\n","def compute_bias_metrics_for_model(dataset,\n","                                   subgroups,\n","                                   model,\n","                                   label_col,\n","                                   include_asegs=False):\n","    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n","    records = []\n","    for subgroup in subgroups:\n","        record = {\n","            'subgroup': subgroup,\n","            'subgroup_size': len(dataset[dataset[subgroup]])\n","        }\n","        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n","        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n","        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n","        records.append(record)\n","    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["bias_metrics_df = compute_bias_metrics_for_model(validate_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n","bias_metrics_df"]},{"cell_type":"markdown","metadata":{},"source":["## Calculate the final score"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["def calculate_overall_auc(df, model_name):\n","    true_labels = df[TOXICITY_COLUMN]\n","    predicted_labels = df[model_name]\n","    return metrics.roc_auc_score(true_labels, predicted_labels)\n","\n","def power_mean(series, p):\n","    total = sum(np.power(series, p))\n","    return np.power(total / len(series), 1 / p)\n","\n","def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n","    bias_score = np.average([\n","        power_mean(bias_df[SUBGROUP_AUC], POWER),\n","        power_mean(bias_df[BPSN_AUC], POWER),\n","        power_mean(bias_df[BNSP_AUC], POWER)\n","    ])\n","    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)    "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["get_final_metric(bias_metrics_df, calculate_overall_auc(validate_df, MODEL_NAME))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":1375107,"sourceId":12500,"sourceType":"competition"},{"datasetId":1835,"sourceId":3176,"sourceType":"datasetVersion"}],"dockerImageVersionId":23026,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
