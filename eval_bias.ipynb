{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from metrics import compute_our_bias_metrics_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "MODEL_LIST = ['cnn','roberta-base-unbiased','roberta-base-unbiased-small','kaggle_bert','my_kaggle_bert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "TOXICITY_COLUMN = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "    \n",
    "def convert_dataframe_to_bool(df, MODEL_NAME):\n",
    "    bool_df = df.copy()\n",
    "    for col in ['target', MODEL_NAME] + identity_columns: # no target here\n",
    "        convert_to_bool(bool_df, col)\n",
    "    return bool_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demographic Parity and Equalized Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for MODEL_NAME in MODEL_LIST:\n",
    "    if not os.path.exists(f'./bias/{MODEL_NAME}_bias.csv'):\n",
    "        eval_df = pd.read_csv(f'./submissions/{MODEL_NAME}_submission.csv')\n",
    "        eval_df = convert_dataframe_to_bool(eval_df, MODEL_NAME)\n",
    "        our_bias_df = compute_our_bias_metrics_for_model(eval_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "        results[MODEL_NAME] = our_bias_df\n",
    "        our_bias_df.to_csv(f'./bias/{MODEL_NAME}_bias.csv', index=False)\n",
    "    else:\n",
    "        our_bias_df = pd.read_csv(f'./bias/{MODEL_NAME}_bias.csv')\n",
    "        results[MODEL_NAME] = our_bias_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>demographics_parity</th>\n",
       "      <th>equalized_opportunity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>christian</td>\n",
       "      <td>2109</td>\n",
       "      <td>0.035562</td>\n",
       "      <td>0.280952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>2602</td>\n",
       "      <td>0.051499</td>\n",
       "      <td>0.317143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>2112</td>\n",
       "      <td>0.068655</td>\n",
       "      <td>0.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jewish</td>\n",
       "      <td>411</td>\n",
       "      <td>0.070560</td>\n",
       "      <td>0.308824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>muslim</td>\n",
       "      <td>1054</td>\n",
       "      <td>0.081594</td>\n",
       "      <td>0.270386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>538</td>\n",
       "      <td>0.104089</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>238</td>\n",
       "      <td>0.126050</td>\n",
       "      <td>0.462963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>1178</td>\n",
       "      <td>0.140917</td>\n",
       "      <td>0.345609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>black</td>\n",
       "      <td>761</td>\n",
       "      <td>0.143233</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  demographics_parity  \\\n",
       "0                      christian           2109             0.035562   \n",
       "1                         female           2602             0.051499   \n",
       "2                           male           2112             0.068655   \n",
       "3                         jewish            411             0.070560   \n",
       "4                         muslim           1054             0.081594   \n",
       "5      homosexual_gay_or_lesbian            538             0.104089   \n",
       "6  psychiatric_or_mental_illness            238             0.126050   \n",
       "7                          white           1178             0.140917   \n",
       "8                          black            761             0.143233   \n",
       "\n",
       "   equalized_opportunity  \n",
       "0               0.280952  \n",
       "1               0.317143  \n",
       "2               0.343750  \n",
       "3               0.308824  \n",
       "4               0.270386  \n",
       "5               0.227273  \n",
       "6               0.462963  \n",
       "7               0.345609  \n",
       "8               0.333333  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_res = results['cnn']\n",
    "model_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0514988470407378"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = model_res[model_res['subgroup']=='female']['demographics_parity']\n",
    "val.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "genders = ['male', 'female', 'homosexual_gay_or_lesbian']\n",
    "religions = ['christian', 'jewish', 'muslim']\n",
    "races = ['black', 'white']\n",
    "illnesses = ['psychiatric_or_mental_illness']\n",
    "groups = [genders, religions, races, illnesses]\n",
    "groups_name = ['genders', 'religions', 'races', 'illnesses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pairwise Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "demo_pairity_dict = {}\n",
    "equal_opportunity_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################## demographics_parity ##################\n",
      "\n",
      "\n",
      "============= cnn =============\n",
      "\n",
      "\n",
      "genders\n",
      "--------\n",
      "max difference: ('female', 'homosexual_gay_or_lesbian') | 0.0526\n",
      "min difference: ('male', 'female') | 0.0172\n",
      "smallest ratio: ('female', 'homosexual_gay_or_lesbian', 0.4947567804985168)\n",
      "largest ratio: ('female', 'male', 0.7501073444830227)\n",
      "\n",
      "religions\n",
      "--------\n",
      "max difference: ('christian', 'muslim') | 0.0460\n",
      "min difference: ('jewish', 'muslim') | 0.0110\n",
      "smallest ratio: ('christian', 'muslim', 0.43583975652519097)\n",
      "largest ratio: ('jewish', 'muslim', 0.8647654614383528)\n",
      "\n",
      "races\n",
      "--------\n",
      "max difference: ('black', 'white') | 0.0023\n",
      "min difference: ('black', 'white') | 0.0023\n",
      "smallest ratio: ('white', 'black', 0.9838320275385117)\n",
      "largest ratio: ('white', 'black', 0.9838320275385117)\n",
      "\n",
      "illnesses\n",
      "---------\n",
      "psychiatric_or_mental_illness: 0.1261\n",
      "\n",
      "\n",
      "============= roberta-base-unbiased =============\n",
      "\n",
      "\n",
      "genders\n",
      "--------\n",
      "max difference: ('female', 'homosexual_gay_or_lesbian') | 0.0852\n",
      "min difference: ('male', 'female') | 0.0244\n",
      "smallest ratio: ('female', 'homosexual_gay_or_lesbian', 0.4729430058222235)\n",
      "largest ratio: ('female', 'male', 0.7583332431174286)\n",
      "\n",
      "religions\n",
      "--------\n",
      "max difference: ('christian', 'muslim') | 0.0650\n",
      "min difference: ('jewish', 'muslim') | 0.0228\n",
      "smallest ratio: ('christian', 'muslim', 0.5173602067597618)\n",
      "largest ratio: ('jewish', 'muslim', 0.8307460333778828)\n",
      "\n",
      "races\n",
      "--------\n",
      "max difference: ('black', 'white') | 0.0186\n",
      "min difference: ('black', 'white') | 0.0186\n",
      "smallest ratio: ('white', 'black', 0.9138680690711831)\n",
      "largest ratio: ('white', 'black', 0.9138680690711831)\n",
      "\n",
      "illnesses\n",
      "---------\n",
      "psychiatric_or_mental_illness: 0.2017\n",
      "\n",
      "\n",
      "============= roberta-base-unbiased-small =============\n",
      "\n",
      "\n",
      "genders\n",
      "--------\n",
      "max difference: ('female', 'homosexual_gay_or_lesbian') | 0.1281\n",
      "min difference: ('male', 'female') | 0.0363\n",
      "smallest ratio: ('female', 'homosexual_gay_or_lesbian', 0.36183704842428877)\n",
      "largest ratio: ('female', 'male', 0.6669919459947196)\n",
      "\n",
      "religions\n",
      "--------\n",
      "max difference: ('christian', 'muslim') | 0.0935\n",
      "min difference: ('christian', 'jewish') | 0.0413\n",
      "smallest ratio: ('christian', 'muslim', 0.36434329065908005)\n",
      "largest ratio: ('jewish', 'muslim', 0.645255474452555)\n",
      "\n",
      "races\n",
      "--------\n",
      "max difference: ('black', 'white') | 0.0048\n",
      "min difference: ('black', 'white') | 0.0048\n",
      "smallest ratio: ('white', 'black', 0.9755187383917685)\n",
      "largest ratio: ('white', 'black', 0.9755187383917685)\n",
      "\n",
      "illnesses\n",
      "---------\n",
      "psychiatric_or_mental_illness: 0.2017\n",
      "\n",
      "\n",
      "============= kaggle_bert =============\n",
      "\n",
      "\n",
      "genders\n",
      "--------\n",
      "max difference: ('female', 'homosexual_gay_or_lesbian') | 0.0752\n",
      "min difference: ('male', 'female') | 0.0222\n",
      "smallest ratio: ('female', 'homosexual_gay_or_lesbian', 0.5068240190472617)\n",
      "largest ratio: ('female', 'male', 0.7768968925002747)\n",
      "\n",
      "religions\n",
      "--------\n",
      "max difference: ('christian', 'muslim') | 0.0693\n",
      "min difference: ('jewish', 'muslim') | 0.0174\n",
      "smallest ratio: ('christian', 'muslim', 0.4204354730670516)\n",
      "largest ratio: ('jewish', 'muslim', 0.8548256285482564)\n",
      "\n",
      "races\n",
      "--------\n",
      "max difference: ('black', 'white') | 0.0041\n",
      "min difference: ('black', 'white') | 0.0041\n",
      "smallest ratio: ('black', 'white', 0.97605561777024)\n",
      "largest ratio: ('black', 'white', 0.97605561777024)\n",
      "\n",
      "illnesses\n",
      "---------\n",
      "psychiatric_or_mental_illness: 0.1597\n",
      "\n",
      "\n",
      "============= my_kaggle_bert =============\n",
      "\n",
      "\n",
      "genders\n",
      "--------\n",
      "max difference: ('female', 'homosexual_gay_or_lesbian') | 0.1118\n",
      "min difference: ('male', 'female') | 0.0236\n",
      "smallest ratio: ('female', 'homosexual_gay_or_lesbian', 0.4812610988894485)\n",
      "largest ratio: ('female', 'male', 0.8147007306361423)\n",
      "\n",
      "religions\n",
      "--------\n",
      "max difference: ('christian', 'muslim') | 0.0921\n",
      "min difference: ('jewish', 'muslim') | 0.0453\n",
      "smallest ratio: ('christian', 'muslim', 0.4486508039139616)\n",
      "largest ratio: ('jewish', 'muslim', 0.7285445697854457)\n",
      "\n",
      "races\n",
      "--------\n",
      "max difference: ('black', 'white') | 0.0052\n",
      "min difference: ('black', 'white') | 0.0052\n",
      "smallest ratio: ('black', 'white', 0.9793236611333098)\n",
      "largest ratio: ('black', 'white', 0.9793236611333098)\n",
      "\n",
      "illnesses\n",
      "---------\n",
      "psychiatric_or_mental_illness: 0.1975\n",
      "################## equalized_opportunity ##################\n",
      "\n",
      "\n",
      "============= cnn =============\n",
      "\n",
      "\n",
      "genders\n",
      "--------\n",
      "max difference: ('male', 'homosexual_gay_or_lesbian') | 0.1165\n",
      "min difference: ('male', 'female') | 0.0266\n",
      "smallest ratio: ('homosexual_gay_or_lesbian', 'male', 0.6611570247933882)\n",
      "largest ratio: ('female', 'male', 0.9225974025974025)\n",
      "\n",
      "religions\n",
      "--------\n",
      "max difference: ('jewish', 'muslim') | 0.0384\n",
      "min difference: ('christian', 'muslim') | 0.0106\n",
      "smallest ratio: ('muslim', 'jewish', 0.8755364806866953)\n",
      "largest ratio: ('muslim', 'christian', 0.9623917945733618)\n",
      "\n",
      "races\n",
      "--------\n",
      "max difference: ('black', 'white') | 0.0123\n",
      "min difference: ('black', 'white') | 0.0123\n",
      "smallest ratio: ('black', 'white', 0.96448087431694)\n",
      "largest ratio: ('black', 'white', 0.96448087431694)\n",
      "\n",
      "illnesses\n",
      "---------\n",
      "psychiatric_or_mental_illness: 0.4630\n",
      "\n",
      "\n",
      "============= roberta-base-unbiased =============\n",
      "\n",
      "\n",
      "genders\n",
      "--------\n",
      "max difference: ('male', 'homosexual_gay_or_lesbian') | 0.0943\n",
      "min difference: ('male', 'female') | 0.0340\n",
      "smallest ratio: ('homosexual_gay_or_lesbian', 'male', 0.8102589234664707)\n",
      "largest ratio: ('female', 'male', 0.9315363881401616)\n",
      "\n",
      "religions\n",
      "--------\n",
      "max difference: ('christian', 'muslim') | 0.0613\n",
      "min difference: ('christian', 'jewish') | 0.0052\n",
      "smallest ratio: ('muslim', 'christian', 0.875036459852494)\n",
      "largest ratio: ('jewish', 'christian', 0.9894346087949745)\n",
      "\n",
      "races\n",
      "--------\n",
      "max difference: ('black', 'white') | 0.0124\n",
      "min difference: ('black', 'white') | 0.0124\n",
      "smallest ratio: ('black', 'white', 0.9754498949483876)\n",
      "largest ratio: ('black', 'white', 0.9754498949483876)\n",
      "\n",
      "illnesses\n",
      "---------\n",
      "psychiatric_or_mental_illness: 0.7593\n",
      "\n",
      "\n",
      "============= roberta-base-unbiased-small =============\n",
      "\n",
      "\n",
      "genders\n",
      "--------\n",
      "max difference: ('male', 'female') | 0.0715\n",
      "min difference: ('male', 'homosexual_gay_or_lesbian') | 0.0214\n",
      "smallest ratio: ('female', 'male', 0.8661654135338344)\n",
      "largest ratio: ('homosexual_gay_or_lesbian', 'male', 0.959975696817802)\n",
      "\n",
      "religions\n",
      "--------\n",
      "max difference: ('jewish', 'muslim') | 0.0812\n",
      "min difference: ('christian', 'jewish') | 0.0367\n",
      "smallest ratio: ('jewish', 'muslim', 0.8248910675381262)\n",
      "largest ratio: ('jewish', 'christian', 0.9124331550802138)\n",
      "\n",
      "races\n",
      "--------\n",
      "max difference: ('black', 'white') | 0.0230\n",
      "min difference: ('black', 'white') | 0.0230\n",
      "smallest ratio: ('black', 'white', 0.9539218403547671)\n",
      "largest ratio: ('black', 'white', 0.9539218403547671)\n",
      "\n",
      "illnesses\n",
      "---------\n",
      "psychiatric_or_mental_illness: 0.6852\n",
      "\n",
      "\n",
      "============= kaggle_bert =============\n",
      "\n",
      "\n",
      "genders\n",
      "--------\n",
      "max difference: ('male', 'homosexual_gay_or_lesbian') | 0.0813\n",
      "min difference: ('male', 'female') | 0.0140\n",
      "smallest ratio: ('homosexual_gay_or_lesbian', 'male', 0.8363963080944213)\n",
      "largest ratio: ('female', 'male', 0.9717879604672057)\n",
      "\n",
      "religions\n",
      "--------\n",
      "max difference: ('jewish', 'muslim') | 0.0420\n",
      "min difference: ('christian', 'muslim') | 0.0056\n",
      "smallest ratio: ('muslim', 'jewish', 0.9047210300429184)\n",
      "largest ratio: ('muslim', 'christian', 0.9861146175208281)\n",
      "\n",
      "races\n",
      "--------\n",
      "max difference: ('black', 'white') | 0.0142\n",
      "min difference: ('black', 'white') | 0.0142\n",
      "smallest ratio: ('black', 'white', 0.9685975609756098)\n",
      "largest ratio: ('black', 'white', 0.9685975609756098)\n",
      "\n",
      "illnesses\n",
      "---------\n",
      "psychiatric_or_mental_illness: 0.6111\n",
      "\n",
      "\n",
      "============= my_kaggle_bert =============\n",
      "\n",
      "\n",
      "genders\n",
      "--------\n",
      "max difference: ('male', 'homosexual_gay_or_lesbian') | 0.0904\n",
      "min difference: ('male', 'female') | 0.0226\n",
      "smallest ratio: ('homosexual_gay_or_lesbian', 'male', 0.848575508261372)\n",
      "largest ratio: ('female', 'male', 0.9621540762902019)\n",
      "\n",
      "religions\n",
      "--------\n",
      "max difference: ('christian', 'jewish') | 0.0333\n",
      "min difference: ('christian', 'muslim') | 0.0140\n",
      "smallest ratio: ('jewish', 'christian', 0.9375)\n",
      "largest ratio: ('muslim', 'christian', 0.9737124463519312)\n",
      "\n",
      "races\n",
      "--------\n",
      "max difference: ('black', 'white') | 0.0072\n",
      "min difference: ('black', 'white') | 0.0072\n",
      "smallest ratio: ('black', 'white', 0.9874225227400789)\n",
      "largest ratio: ('black', 'white', 0.9874225227400789)\n",
      "\n",
      "illnesses\n",
      "---------\n",
      "psychiatric_or_mental_illness: 0.7407\n"
     ]
    }
   ],
   "source": [
    "for metric in ['demographics_parity','equalized_opportunity']:\n",
    "    print(f'################## {metric} ##################')\n",
    "    for model_name, model_res in results.items():\n",
    "        print('\\n\\n=============',model_name,'=============\\n\\n')\n",
    "        for idx, group in enumerate(groups): # for all sensitive groups\n",
    "            score_dict = {} # dict for storing scores\n",
    "            for subgroup in group:\n",
    "                val = model_res[model_res['subgroup']==subgroup][metric].values[0]\n",
    "                score_dict[subgroup] = val\n",
    "            if len(group) <= 1: # for illneses\n",
    "                print(groups_name[idx])\n",
    "                print('---------')\n",
    "                print(f'{group[0]}: {score_dict[group[0]]:.4f}')\n",
    "                demo_pairity_dict[(model_name, groups_name[idx])] = (score_dict[group[0]],score_dict[group[0]],score_dict[group[0]])\n",
    "                equal_opportunity_dict[(model_name, groups_name[idx])] = (score_dict[group[0]],score_dict[group[0]],score_dict[group[0]])\n",
    "                continue\n",
    "            ratios = []\n",
    "            for i in range(len(group)):\n",
    "                for j in range(i+1,len(group)):\n",
    "                    if score_dict[group[i]] < score_dict[group[j]]:\n",
    "                        temp = score_dict[group[i]]/score_dict[group[j]]\n",
    "                        ratios.append((group[i],group[j], temp))\n",
    "                    else:\n",
    "                        temp = score_dict[group[j]]/score_dict[group[i]]\n",
    "                        ratios.append((group[j],group[i], temp))\n",
    "            max_diff = float('-inf')\n",
    "            min_diff = float('inf')\n",
    "\n",
    "            for key1, value1 in score_dict.items():\n",
    "                for key2, value2 in score_dict.items():\n",
    "                    if key1 != key2:\n",
    "                        diff = abs(value1 - value2)\n",
    "                        if diff > max_diff:\n",
    "                            max_diff = diff\n",
    "                            max_diff_keys = (key1, key2)\n",
    "                        if diff < min_diff:\n",
    "                            min_diff = diff\n",
    "                            min_diff_keys = (key1, key2)\n",
    "\n",
    "            # print(max_diff_keys, min_diff_keys)\n",
    "            # print(max_diff, min_diff)\n",
    "            smallest_ratio = min(ratios, key=lambda x: x[2])\n",
    "            largest_ratio = max(ratios, key=lambda x: x[2])\n",
    "            print(groups_name[idx])\n",
    "            print('--------')\n",
    "            print(f'max difference: {max_diff_keys} | {max_diff:.4f}')\n",
    "            print(f'min difference: {min_diff_keys} | {min_diff:.4f}')\n",
    "            print(f'smallest ratio: {smallest_ratio}')\n",
    "            print(f'largest ratio: {largest_ratio}')\n",
    "            print()\n",
    "            if metric == 'demographics_parity':\n",
    "                demo_pairity_dict[(model_name, groups_name[idx])] = (smallest_ratio[2], largest_ratio[2],(smallest_ratio[2]+largest_ratio[2])/2)\n",
    "            elif metric == 'equalized_opportunity':\n",
    "                equal_opportunity_dict[(model_name, groups_name[idx])] = (smallest_ratio[2], largest_ratio[2],(smallest_ratio[2]+largest_ratio[2])/2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models bias in different groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographic parity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== genders =======\n",
      "\n",
      "                           \tsmallest ratio   |   largest ratio   |   Average\n",
      "cnn                        : (0.4947567804985168, 0.7501073444830227, 0.6224320624907698)\n",
      "roberta-base-unbiased      : (0.4729430058222235, 0.7583332431174286, 0.615638124469826)\n",
      "roberta-base-unbiased-small: (0.36183704842428877, 0.6669919459947196, 0.5144144972095042)\n",
      "kaggle_bert                : (0.5068240190472617, 0.7768968925002747, 0.6418604557737682)\n",
      "my_kaggle_bert             : (0.4812610988894485, 0.8147007306361423, 0.6479809147627954)\n",
      "\n",
      "======== religions =======\n",
      "\n",
      "                           \tsmallest ratio   |   largest ratio   |   Average\n",
      "cnn                        : (0.43583975652519097, 0.8647654614383528, 0.6503026089817718)\n",
      "roberta-base-unbiased      : (0.5173602067597618, 0.8307460333778828, 0.6740531200688222)\n",
      "roberta-base-unbiased-small: (0.36434329065908005, 0.645255474452555, 0.5047993825558176)\n",
      "kaggle_bert                : (0.4204354730670516, 0.8548256285482564, 0.637630550807654)\n",
      "my_kaggle_bert             : (0.4486508039139616, 0.7285445697854457, 0.5885976868497036)\n",
      "\n",
      "======== races =======\n",
      "\n",
      "                           \tsmallest ratio   |   largest ratio   |   Average\n",
      "cnn                        : (0.9838320275385117, 0.9838320275385117, 0.9838320275385117)\n",
      "roberta-base-unbiased      : (0.9138680690711831, 0.9138680690711831, 0.9138680690711831)\n",
      "roberta-base-unbiased-small: (0.9755187383917685, 0.9755187383917685, 0.9755187383917685)\n",
      "kaggle_bert                : (0.97605561777024, 0.97605561777024, 0.97605561777024)\n",
      "my_kaggle_bert             : (0.9793236611333098, 0.9793236611333098, 0.9793236611333098)\n",
      "\n",
      "======== illnesses =======\n",
      "\n",
      "                           \tsmallest ratio   |   largest ratio   |   Average\n",
      "cnn                        : (0.4629629629629629, 0.4629629629629629, 0.4629629629629629)\n",
      "roberta-base-unbiased      : (0.7592592592592593, 0.7592592592592593, 0.7592592592592593)\n",
      "roberta-base-unbiased-small: (0.6851851851851852, 0.6851851851851852, 0.6851851851851852)\n",
      "kaggle_bert                : (0.6111111111111112, 0.6111111111111112, 0.6111111111111112)\n",
      "my_kaggle_bert             : (0.7407407407407407, 0.7407407407407407, 0.7407407407407407)\n"
     ]
    }
   ],
   "source": [
    "max_len = len('roberta-base-unbiased-small')\n",
    "for idx, group in enumerate(groups):\n",
    "    print(f'\\n======== {groups_name[idx]} =======\\n')\n",
    "    spaces = 3\n",
    "    print(f\"{' ' * max_len}\\tsmallest ratio{'':>{spaces}}|{'':>{spaces}}largest ratio{'':>{spaces}}|{'':>{spaces}}Average\")\n",
    "    for model_name in MODEL_LIST:\n",
    "        print(f'{model_name.ljust(max_len)}: ', end='')\n",
    "        print(f'{str(demo_pairity_dict[(model_name, groups_name[idx])]):<}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equalized Opportunity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== genders =======\n",
      "\n",
      "                           \tsmallest ratio   |   largest ratio   |   Average\n",
      "cnn                        : (0.6611570247933882, 0.9225974025974025, 0.7918772136953953)\n",
      "roberta-base-unbiased      : (0.8102589234664707, 0.9315363881401616, 0.8708976558033161)\n",
      "roberta-base-unbiased-small: (0.8661654135338344, 0.959975696817802, 0.9130705551758183)\n",
      "kaggle_bert                : (0.8363963080944213, 0.9717879604672057, 0.9040921342808135)\n",
      "my_kaggle_bert             : (0.848575508261372, 0.9621540762902019, 0.9053647922757869)\n",
      "\n",
      "======== religions =======\n",
      "\n",
      "                           \tsmallest ratio   |   largest ratio   |   Average\n",
      "cnn                        : (0.8755364806866953, 0.9623917945733618, 0.9189641376300286)\n",
      "roberta-base-unbiased      : (0.875036459852494, 0.9894346087949745, 0.9322355343237343)\n",
      "roberta-base-unbiased-small: (0.8248910675381262, 0.9124331550802138, 0.86866211130917)\n",
      "kaggle_bert                : (0.9047210300429184, 0.9861146175208281, 0.9454178237818732)\n",
      "my_kaggle_bert             : (0.9375, 0.9737124463519312, 0.9556062231759657)\n",
      "\n",
      "======== races =======\n",
      "\n",
      "                           \tsmallest ratio   |   largest ratio   |   Average\n",
      "cnn                        : (0.96448087431694, 0.96448087431694, 0.96448087431694)\n",
      "roberta-base-unbiased      : (0.9754498949483876, 0.9754498949483876, 0.9754498949483876)\n",
      "roberta-base-unbiased-small: (0.9539218403547671, 0.9539218403547671, 0.9539218403547671)\n",
      "kaggle_bert                : (0.9685975609756098, 0.9685975609756098, 0.9685975609756098)\n",
      "my_kaggle_bert             : (0.9874225227400789, 0.9874225227400789, 0.9874225227400789)\n",
      "\n",
      "======== illnesses =======\n",
      "\n",
      "                           \tsmallest ratio   |   largest ratio   |   Average\n",
      "cnn                        : (0.4629629629629629, 0.4629629629629629, 0.4629629629629629)\n",
      "roberta-base-unbiased      : (0.7592592592592593, 0.7592592592592593, 0.7592592592592593)\n",
      "roberta-base-unbiased-small: (0.6851851851851852, 0.6851851851851852, 0.6851851851851852)\n",
      "kaggle_bert                : (0.6111111111111112, 0.6111111111111112, 0.6111111111111112)\n",
      "my_kaggle_bert             : (0.7407407407407407, 0.7407407407407407, 0.7407407407407407)\n"
     ]
    }
   ],
   "source": [
    "max_len = len('roberta-base-unbiased-small')\n",
    "for idx, group in enumerate(groups):\n",
    "    print(f'\\n======== {groups_name[idx]} =======\\n')\n",
    "    spaces = 3\n",
    "    print(f\"{' ' * max_len}\\tsmallest ratio{'':>{spaces}}|{'':>{spaces}}largest ratio{'':>{spaces}}|{'':>{spaces}}Average\")\n",
    "    for model_name in MODEL_LIST:\n",
    "        print(f'{model_name.ljust(max_len)}: ', end='')\n",
    "        print(f'{str(equal_opportunity_dict[(model_name, groups_name[idx])]):<}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
